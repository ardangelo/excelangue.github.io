---
layout: post
title: An Essay on Data Security
categories: [essays]
---

### Foreword

This research essay was written as the final paper for my junior year English class, and as such the terms and phrasing assumes the reader has little-to-some technological knowledge. If for some reason somebody wants to read it, well, here it is.

# Security Implications of Corporate Ownership of Personal Data

In the United States, corporate losses due to computer crime topped $60 billion in 2002 (“Market Reactions”). In 10 years, the figure rose to more than $110 billion (Symantec). Despite unimaginable advancements in technology, crime perpetrated over computer networks is clearly not going away. The increasing proliferation of computing devices has only created more routes of attack for the unscrupulous. Popular questions to ask when designing secure computing systems and creating laws to stop these crimes are whether computer crime is getting worse; in what ways the computer landscape is changing, and if anything can be done to prevent insecure computing. The way that the industry answers these questions is more technology: more devices, more keys, and more complexity. This focus, however, is completely wrong; it should not be on the computer system, but rather the computer user. Systems should be designed with the users in mind, and take into account the habits and behaviors of the average user. Unfortunately, due to the current expectations and pressures of the software industry, as well as laws allowing negligence and oversight in software products, companies are reluctant or unwilling to reinvent huge swaths of their computer security systems to address both long-standing and future threats.

Behind the laws and company policies are the end users, the people who have to deal with the computer systems -- the most lucrative target for attackers seeking to compromise a computer system. Despite end users being most at risk, businesses as a whole are not immune to attack. If anything, they are more vulnerable, being comprised of a larger amount of vulnerable users. However, successful attacks on businesses usually harm the customers of the businesses more than the company itself. Between a lack of corporate liability and misdirected government regulations, users of computer systems operate in an environment in which they are unlikely to be able to effectively defend against waves of cyber-attack from both known and unknown threats. After more than a decade of such consequences as annual billion-dollar losses and millions of stolen consumer records (Armerding), drastic action is the only way to change the industry from its current path of indifference. Meanwhile, corporations seeking to protect secure data often are unwilling to pay the price needed to implement a system that can withstand multitudes of attack from all points.

The computer software industry, quite interestingly, is almost completely unregulated by the government. The only effective form of regulation that the industry has is in the form of “best practice<sup>1</sup>”. It is usually only in cases of devastating human or financial loss that the government steps in, such as in the case of the Therac-25, “a device used to administer radiation treatment to cancer patients” and similar software-controlled devices (“Framework”). Bugs in the control software caused the death of patients treated with the device. A prime example of the allowable lack of liability is found in the ubiquitous End User License Agreement<sup>2</sup>. Microsoft’s Windows 98, earlier one of the most popular pieces of software in the world, included a “draconian” license agreement that must be accepted to use the product, which includes a clause that states: “In no event shall Manufacturer or its suppliers be liable for any damages whatsoever—arising out of the use or of inability to use this product, even if Manufacturer has been advised of the possibility of such damages” (Schneier 365). This single clause absolves Microsoft completely of any legal responsibility to make sure Windows 98 is a secure and reliable product. 

There’s a good reason that heavy-handed legal waivers are not often found on physical products: consumer-protection laws governing the responsibility of manufacturers to produce a safe and reliable product already exist. But because software companies have no effective legal liability, software vendors can get away with a lot more when it comes to dealing with faults in a product. Due to nonexistent legal pressure, the two largest checks on software quality are the buying power of the consumer (although individual negative feedback is unlikely to have an effect on software juggernauts with millions or billions of users such as Microsoft Windows or Office), and pressure internal to the industry: what every other company is doing, also known as “best practice”. 

The software industry is not the only industry that is affected by the lack of software liability. Even though the typical American company may not design or program the software it uses, it still entrusts its day-to-day operations and data to the software. Any kind of large corporation has computerized records; department stores have credit card numbers, banks have social security numbers, and so forth. Based on a listing of the worst computer security breaches since 2000, there are two major targets when a corporate computer system is compromised: the corporation’s sensitive data, and the consumer data contained within the system (Armerding). With computerized data comes the duty to secure this data from outside threats. Unfortunately, the goal of defending vast stores of information often falls short, especially with no legal incentive to pour resources into defense. Without legal pressure, the only effective pressure is that from the economy. Logic would dictate that when a system is compromised, whether consumer data or company data is compromised, either earnings (company data), or stock prices (consumer data) would fall, producing motivation for the company to improve data safeguards. However, actual market trends strongly suggest that this is not the case. Long-term analysis of market data showed that announcement of data breaches has no effect on stock prices or earnings (“Market Reactions"). One notable corporate breach occurred when attackers gained access to VeriSign’s highly restricted signing systems, which contain root certificates<sup>3</sup> that are used to issue keys for web-based encryption (Armerding). An attack on the root certificate would be absolutely devastating for any kind of secure network-based exchange of data. However, revoking VeriSign’s root certificate would require the retooling of every single technology making use of the HTTPS protocol. Despite potential damages, too much effort would be required to reimplement the entire secure Internet, and as a result VeriSign is still signing and issuing certificates, with no effect on the bottom line. Without any significant penalties for consumer data loss, the only remaining threat is that of stolen company data. 

The threat of sensitive company data being compromised has been shown to be largely inconsequential, but not because it has no effect, rather, industrial-espionage type attacks are quite uncommon. In a compilation of the 15 most damaging breaches of the 21st century, only 2 resulted in damage to the compromised company, as opposed to 10 leaking personal consumer information, such as credit card numbers and social security numbers (Armerding). With the large majority of breaches affecting only users and breaches in general not having much effect on earnings, a business looking to save extra money would be hard-pressed to find an economic incentive to spend money on effectively and thoroughly protecting a computer system and the data contained within. In this situation, the loser is the consumer who entrusted the company with personal data; they cannot determine how the company uses or safeguards their data, the only option is to not divulge the information at all, and for the vast majority of consumers, giving up such conveniences as online shopping or paying bills online is too much. Without the vigilant support of the software vendor to make sure that their software is secure and bug-free, the bulk of the computer security falls on the shoulders of the users operating the system to reduce available avenues for attack. Unfortunately, the vendors that create the software are not much better in implementing effective security practices when it comes to designing software products due to huge economic pressure to release a product as soon as possible.

In the world of business, whatever company gets their product out the door first is the company that has the earliest chance to appeal to consumers. The same usually holds true for computer software; the product released sooner has a higher chance of market penetration purely because it is available (“Sell First”). People don’t like changing deeply rooted habits, and relearning an entire software package is too much trouble for the average user. Often, though, products released early contain bugs, as proper testing takes a lot of time and resources (“Sell First”). There is effectively disincentives in taking time to properly test and debug<sup>4</sup> a software product: less market penetration and the resources required to do so. Security expert Bruce Schneier suggests publishing source code to let a large community of enthusiasts review code, but in a commercial product this is unfeasible, not only because it requires an entirely different business model, but also because the time required to effectively debug it is increased drastically. For free and open-source products<sup>5</sup> not tied to the software industry and market, this is often the only solution, as hiring professionals for a free, decentralized product is unlikely to occur, but in a commercial environment where time is money and resources are expensive, companies with no incentives to do otherwise often release products early, and with a minimum of testing. 

With the advent of high-speed broadband Internet connections, patching<sup>6</sup> has rapidly become the preferred method for fixing software, because vendors can release a product as soon as possible and fix the mistakes once they trickle in through bug reports (“Sell First”). However, once a software patch is released, many system administrators don’t even apply the fix, leaving the machines running the buggy software open for attack. Despite the human factors, the larger issue with patching is that even if the patch is applied it is still done entirely after the fact (Schneier 340). The vulnerability must first be discovered, reported, a fix created, and a patch issued. Sun Microsystems, the maintainer of Java, one of the most widely used computer programming languages in the world, took an average of 133 days to patch reported issues in its popular “Solaris” operating system in 2007. The fastest patcher, Microsoft, still took an average of 12 days to issue a software patch (“Threat Report” 25). Even if the best-case scenario is a 12-day patch time there is still the time before and during those days during which an attack can be attempted. In addition, if an attacker finds a vulnerability that can be used for a malevolent purpose, it is safe to say that the attacker won’t be reporting the issue to the software vendor, and is free to exploit the security hole at will. So if skilled evaluation from within the company is limited by time and resource constraints, what about independent evaluation? Under United States law, this is prohibited without express permission of the vendor. And without liability, there is no incentive to allow an analyst to expose flaws in an already released and purchased product. 

In the current software market, where the only effective check on software quality is the buying power of consumers, too much bad press can be deadly. For a vendor that has decided to take the easy way out and release buggy, insecure software, independent evaluation of the software product in question could reveal any number of flaws, and the release of the findings would definitely negatively affect consumer perspective on the software product. Therefore, it is in the best interests of such software vendors to close off systems to outside research, as to not expose flaws in the product. Independent evaluation, however, would be an uncontrollable aspect of code review if not for one thing: the Digital Millennium Copyright Act. The most devastating blow to code security evaluation comes straight from the government, the entity that should be protecting consumers through enforceable regulations instead hampers review of potentially insecure product, by criminalizing unauthorized analysis through reverse engineering<sup>7</sup> of any copyrighted product or computer system that falls under the umbrella of the DCMA (Von Lohmann 1). 
The DCMA was originally meant to give copyright laws more power to enforce compliance on the Internet, but it was worded so vaguely that plaintiffs now use it for everything from lawsuits against those who copy personally owned DVDs to something as unrelated as a “general-purpose prohibition on computer network access” (Von Lohmann 3). As a result, the DCMA has become a tool of corporations as part of an effort not to reveal how flawed their systems are to the general public. The easiest way to find a security vulnerability in a system is to take it apart and see how it works; open-source software relies heavily on this method to make sure flaws are caught and fixed more easily (Schneier 346). But because reverse engineering of a copyrighted product or even conducting an examination of a computer system is a criminal offense, such analysis is illegal. When the DCMA is used to prevent the improvement of a product at the cost of its creators’ reputation, the user is the only person affected negatively, because it is the user actually has to use the buggy software and interact with the system, and not the vendor. Users actually have to place their trust in the solution they are using, and if nobody is allowed to make sure that the solution is safe and effective, then the user must take every claim from the vendor or administrator on faith. The lack of liability means that there is no motivation on the company’s part to ensure that the user’s faith is well placed.

Sometimes, not even the publication of a serious vulnerability will change anything; some companies will simply choose to ignore it. Take the average security token. A token from RSA is one of the most widely used identification methods used for outside access to a secure network. It’s small, smaller than a credit card, and so is easily stolen. Once a thief has access to the token, it is possible to extract the special “key” used to identify the token (and thus the user) to the secure server. RSA relies on the fact that extracting this key is very difficult, even with high-powered computers. However, researchers have found a method to extract this key from the token “in as little as 13 minutes” (Sengupta). By all means, this should be a security nightmare; all an attacker needs is 13 minutes alone with the token and its precious key can be revealed, allowing access to company servers containing all manners of sensitive information. Even worse, the vulnerability was discovered without the use of reverse engineering (as the use of such methods would violate the DCMA and as such would be illegal), meaning that the flaw was quite apparent and widely exploitable. RSA’s reaction to this problem was ridiculously underwhelming, stating that “There are many technologies we abstractly know are problematic [meaning the key exploit] and we prioritize fixing them less than things that are obviously on fire” (Sengupta). For RSA, fixing the key problem would require the recall and reworking of hundreds of thousands, if not millions of already deployed tokens and all the related servers. The official response makes it all too clear that RSA does not plan on patching the vulnerability anytime soon. Companies can choose not to use RSA’s flawed token, but as discussed before, there is no reason to do so. It would only cost more money and more effort to revise the remote identification system, all to protect against a situation that would not harm the company, but the users. This is only one of the many potential situations where liability laws and other positive pressures would protect the consumer and their data instead of harming them through stifling legitimate research. 

The two entities that can effectively protect users from faulty software and insecure systems are the government, through liability laws, and the companies and vendors themselves, by designing and releasing high-quality, secure software and by putting pressure on the rest of the industry to do the same. Unfortunately, both fall short; the government continues to rely on its vague and easily abusable Digital Millennium Copyright Act, corporations have no motivation to adequately secure sensitive customer data, and the software industry is caught in a feedback loop of racing to release software and so forcing lower and lower quality and security in the hurry to ship before anybody else. With no assistance from either party, users are stuck using insecure software and entrusting their personal information to servers implementing incomplete and insecure security solutions. This combination of software and systems makes it extremely easy for an attacker to gain access to the data within the systems, either through the holes in the software or security solution, or more commonly, exploiting the vulnerabilities using the users themselves.

Lacking support from the government and the industry, users are increasingly forced to rely on nothing but their own common sense and what they know in the fight to keep attackers out and data secure. If the average user is taken to be one without significant computer skills (as in, the amount of skill needed to get a job done using a computer), then expecting an average user to be a legitimate defense against a skilled attacker is pointless and foolish. Yet there seems to be no other option. The difficulty can be eased greatly by designing and implementing a simple and easy to understand computer system, but that brings with it its own set of challenges. Like designing a secure computer system, creating a simple yet effective system of any kind takes more resources than a “quick and dirty solution” (Markoff). In an interview, revered computer scientist Peter Neumann stated that a large problem with computer security is that the industry is more concerned with temporary fixes instead of lasting, but more difficult solutions (Markoff). On the opposite end of simple and effective solutions is the complex, piecemeal method preferred for its strong appearance but cheap implementation. 

During the spread of mobile phones in the early 2000’s, a protocol was developed for a sort of content delivery system for mobile phones called Wireless Access Protocol, or WAP. Its goal was an efficient but content-rich experience on mobile phones. In order to reduce the effort needed to gain support for WAP, it was designed with a “gateway” (in reality a large, powerful server) between the phone and the content (for example, a web page) that was maintained by the mobile service provider. This allowed for phone users to have such benefits as compression and formatting done automatically by a server, while content providers had to do very little when it came to actually converting content for consumption by mobile phones (“Security Holes”). On paper, this idea seemed very good, but the intricacies of WAP meant that all data processed by the gateway had to be unencrypted. As WAP was meant primarily for content-consumption, the unencrypted data was more often than not involving credit card numbers or purchase orders. Adding the gateway to WAP increased its success, but it also increased its complexity and insecurity. Having the gateway only process unencrypted data meant that anyone with any kind of access to it could read off all the data that passed through it. WAP featured encryption between the phone and the gateway, and encryption between the gateway and the rest of the Internet, but because of that one break in the chain of security (where the data was decrypted and stored on the gateway), every other security measure becomes pointless. Complexity also has another effect: having a more complex system makes it more difficult to maintain (Schneier 355). The addition of the gateway meant that many skilled technicians had to work on the gateway to ensure it was functioning correctly. Without the gateway, WAP processing maintenance could conceivably be performed by the average user (it would probably be similar to a simple fiber connection), but because of the added complexity of the gateway more people had access to it, and therefore could access the privileged data residing within it.

Removing sensitive data from a protected zone is a good way to allow that data to be compromised. As seen in the case of WAP, the best security scheme can be rendered useless by one broken link in a chain. Yet, removal of data is exactly what is happening in nearly every business through insecure mobile devices, or more generally, “endpoints”. According to Trend Micro’s cloud security vice-president, the entire idea of securing a system the traditional way (through the network) is becoming not only antiquated but also increasingly difficult due to the sheer amount of computing power that is being moved outside the protected zone of the company network to the endpoint (Hickey). Therein lies the vulnerability: if a legitimate user can easily access a secure network, then an attacker could also more easily access the network. In the case of RSA’s tokens which “are used to give … employees access to their networks from home, or while travelling”, the entire vulnerability of token theft could be nullified if not for the fact that the tokens must be taken outside company walls into untrusted territory. An effective system must take into account every link in the security chain and adequately defend against the possibility that the link is compromised. Allowing the data to leave control of the company and to enter the hands of the user extends the chain and creates more attack vectors. Fortunately, major security professionals mentioned in Hickey’s article (representing a wide variety of professionals throughout the computer security industry) realize the importance of endpoint security, and are working on implementing it into corporate solutions. However, as one of the specialists notes, the realities of limited budget often forces companies to choose between the two types of solutions. An effective traditional centralized security system leaves the endpoints vulnerable to attack, but having an effective endpoint solution can leave the central network inadequately unprotected (Hickey). For system architects, having only enough in the budget to cater to one type of defense (network or endpoint) means that either the network or the endpoints remain inadequately protected. As transitioning from network-central security to endpoint-central security takes as much effort and as many resources as a complete system redesign (as the two types are completely different in implementation), companies that do not provide adequate security in the first place are unlikely to do the same to make sure endpoints are secure as well. Every link in the chain must function for the implementation to work, from the building security to the people that work on the sensitive machines, and leaving large sections of the chain vulnerable can have devastating effects for whatever the system is trying to protect. Based on Armerding’s compilation of the most destructive attacks of the 21st century, it’s the consumer’s personal data that is being shielded by the links in the security chain.

One of the most lucrative targets for an attacker looking to compromise a secure system is the user of the system. The methods used can differ greatly, from the need to get work done to exploitation of obviously unreviewed code. The variable in a secure system is often how well the system is designed and what its specific flaws are, but one constant is that there is a user that needs the system to do something, and as such has access to it. “It is … possible to compromise security at a given layer [of a system] by attacking a layer below. For example, the built-in encryption functions in a word processor don’t matter if an attacker can compromise the underlying operating system” (Schneier 128). Extending the layer metaphor, the logical final layer would be the physical environment the secure machine is running in. By gaining access physically, an attacker basically has free reign over the system, from causing downtime by disabling machines to stealing data on disk. When coupled with inadequate protections, even more power is handed to the successful attacker, as was the case with WAP. The fact that the WAP gateway did not encrypt sensitive data meant that in the case of a breach every scrap of data that was processed by the gateway could be leaked (“Security Holes”). Sometimes, though, the attacker doesn’t need to expend effort to physically gain access to the system. 

The technique of manipulating unsuspecting users is called “social engineering”, used in 3 of the 15 “worst data security breaches of the 21st century” (Armerding). Another three attacks were done through malware<sup>8</sup>, all of which would require tricking a user inside the system with privileged access into installing or otherwise using the software. Combining the attacker and the privileged user results in what is known as an “insider”. Insiders are extremely dangerous, as they have privileged access to whatever is being secured, and don’t have social engineering’s disadvantage of needing to convince a legitimate user to unsuspectingly do their bidding (Schneier 48). Going back to the WAP protocol, an insider could decimate the system, or more probably, undetectably skim any amount of personal data flowing through the gateway. Attacks from an insider, whether a malicious user or merely a legitimate user under the control of an attacker, should be planned for and adequately defended against. However, like any improvement that requires redesigning a system to make it more secure, companies are often not willing to expend the effort to do so. As a result, the threat of insider access is downplayed: in a survey of security managers, the majority “perceived a very low level of risk with their internal networks”, despite being informed of the dangers by security advisors (“Threats to Information Systems”). It would seem that when faced with a legitimate problem, whatever administrative body decides security protocols in companies are actively ignoring advice because it is the best decision to make economically. In the current environment the only losers when it comes to bad corporate security are the consumers who trust the companies to safeguard their personal data.

When the abysmal state of corporate computer security is taken into consideration, it is clear that something extreme must happen for the security landscape to improve. Given the reluctance in the industry to put money and resources into overhauling security infrastructure, government intervention seems to be the only way to change the way sensitive computers and consumer data is handled within any acceptable timeframe. Two changes to the legal environment would accomplish most of what is needed to ensure that companies entrusted with data treat it with care. Von Lohmann suggest the repeal of the DCMA, while Schneier pushes for the introduction of software and computer security liability laws. Each major legal change would cause companies to do two things: to make unauthorized analysis possible (prevented by the DCMA), and to force a major overhaul in the way computer security is planned and executed in a corporate environment by creating actual economic penalties for disregarding proper computer defense on the corporate scale. With the onset of software liability, there would be a very strong incentive to thoroughly test and debug products before shipping. Enforcing security and software liability will clean up the sloppy security already found in solutions, and allowing unauthorized analysis of a system has the potential to clear up any bugs that might remain. Above all, these two changes will ultimately benefit the users of these systems, creating an environment where personal data can be safeguarded effectively, with penalties for intentional oversight leading to leaking of sensitive data. The only obstacle remaining is the implementation of these changes; both require support and backing not in the technical or business arena, but in the legislative arena. Only time will tell whether lawmakers will eventually comprehend the current and future state of computer security and bring about these much-needed legal changes.

The bigger picture when analyzing the current state of computer security is effectively a reflection on industry as a whole, and the responsibility of the government to protect the consumer, instead of enabling the corporations. It is apparent that the computer security industry, without any effective pressures to produce quality software, to provide services securely, or to properly safeguard sensitive consumer data, is bound to continue on its path full of catastrophic failures. The government has the responsibility to improve the situation for consumers through regulations, but given the current environment in which the consumer is powerless to make complex system and software decisions it is doubtful if such changes will occur. Instead, heavy-handed laws such as the DCMA are kept on the books, allowing unscrupulous vendors and corporations to release poorly designed and insecure systems without possibility of discovery. Ultimately, lack of control of the computer security industry only hurts every consumer that places their trust in the government to regulate the companies that collect, store, and lose their personal information.

### Footnotes

1. Widely accepted and proven methods for accomplishing a task or set of goals across an industry.
2. A legal document that users must agree to before using most commercial software products, which is infamously dense and long-winded.
3. A certificate lists the unique “fingerprint” of a remote machine so that anyone who connects to it can be sure that they are really connected to the machine that they think they are. VeriSign is in charge of putting a special digital seal on the certificates to make sure that any tampering with the fingerprint renders the certificate invalid.
4. The process of rigorously testing a software program to make sure that it is stable and free of defect.
5. Software whose source code is made public. As a result, it is free to use, to modify and to analyze.
6. The process of automatically upgrading software, most commonly to fix software defects.
7. Disassembly and/or partial recreation of a product to determine exactly how it functions.
8. Computer viruses or software with a damaging effect.

## Works Cited

Anderson, Ross, and Tyler Moore. "Information Security: Where Computer Science, Economics and Psychology Meet." Philosophical Transactions: Mathematical, Physical and Engineering Sciences 367.1898 (2009): 2717-27. JSTOR. Web. 5 Apr. 2013. 

Armerding, Taylor. "The 15 worst data security breaches of the 21st Century." CSO Online. Ed. Derek Slater. IDG Enterprise, 15 Feb. 2012. Web. 19 Apr. 2013. <http://www.csoonline.com/article/700263/the-15-worst-data-security-breaches-of-the-21st-century>. 

Arora, Ashish, Jonathan P. Caulkins, and Rahul Telang. "Research Note: Sell First, Fix Later: Impact of Patching on Software Quality." Management Science 52.3 (2006): 465-71. JSTOR. Web. 1 May 2013. 

Bordoloi, Buoy, Kathleen Mykytyn, and Peter P. Mykytyn, Jr. "A Framework to Limit System Developers' Legal Liabilities." Journal of Management Information Systems 12.4 (1996): 161-85. JSTOR. Web. 25 Apr. 2013. 

Hickey, Andrew R. "Don't Take Endpoints for Granted." CRNtech 1 June 2011: n. pag. LexisNexis. Web. 3 Apr. 2013. 

Juul, Niels Christian, and Niels Jørgensen. "The Security Hole in WAP: An Analysis of the Network and Business Rationales Underlying a Failure." International Journal of Electronic Commerce 7.4 (2003): n. pag. JSTOR. Web. 3 Apr. 2013. 

Kannan, Karthik, Jackie Rees, and Sanjay Sridhar. "Market Reactions to Information Security Breach Announcements: An Empirical Analysis." International Journal of Electronic Commerce 12.1 (2007): 69-91. JSTOR. Web. 3 Apr. 2013. 

Loch, Karen D., Houston H. Carr, and Merrill E. Warkentin. "Threats to Information Systems: Today's Reality, Yesterday's Understanding." MIS Quarterly 16.2 (1992): 173-86. JSTOR. Web. 25 Apr. 2013. 

Markoff, John. "Killing the Computer to Save It." New York Times 30 10 2012, late ed., sec. D: 1. LexisNexis. Web. 17 Apr. 2013. 

Press, Jordan. "Government Urged to Set Cyber Standards." The Gazette [Montreal] 22 Feb. 2013, Final ed., NEWS: A3. LexisNexis. Web. 3 Apr. 2013. 

Schneier, Bruce. Secrets and Lies: Digital Security in a Networked World. Indianapolis: Wiley, 2004. Print. PB 

Sengupta, Somini. "Scientists Make Short Work of Breaking Security Keys." New York Times 26 June 2012, late ed., sec. B: 2. LexisNexis. Web. 25 Apr. 2013. 

Symantec. "Symantec Global Internet Security Threat Report." Symantec Enterprise Security. Ed. Eric Johnson. Symantec, Apr. 2008. Web. 19 May 2013. 

Symantec. 2012 Norton Study: Consumer Cybercrime Estimated at $110 Billion Annually. Symantec. Symantec, 5 Sept. 2012. Web. 19 May 2013. 

Von Lohmann, Fred. "Unintended Consequences: Twelve Years under the DCMA." Electronic Frontier Foundation. N.p., Feb. 2010. Web. 24 Apr. 2013. 

Westland, Chris. "A Rational Choice Model of Computer and Network Crime." International Journal of Electronic Commerce 1.2 (1996): 109-26. JSTOR. Web. 17 Apr. 2013. Westland, Chris. "A Rational Choice Model of Computer and Network Crime." International Journal of Electronic Commerce 1.2 (1996): 109-26. JSTOR. Web. 17 Apr. 2013. 